{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a chatbot with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001AD1A947320>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001AD1A947E60>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Guru! It's nice to meet you.  \\n\\nThat's awesome that you're a data scientist. What kind of work do you specialize in? \\n\\nI'm always interested in learning more about how people use data to solve problems and make insights.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 19, 'total_tokens': 78, 'completion_time': 0.107272727, 'prompt_time': 8.034e-05, 'queue_time': 0.020264169999999998, 'total_time': 0.107353067}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d51881fe-de9d-4d15-9dfd-46fc5569ea21-0', usage_metadata={'input_tokens': 19, 'output_tokens': 59, 'total_tokens': 78})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi I am guru and I am a data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Guru, and you're a data scientist! üòä  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm happy to chat.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 97, 'total_tokens': 143, 'completion_time': 0.083636364, 'prompt_time': 0.0034427, 'queue_time': 0.020834526, 'total_time': 0.087079064}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c8e680da-42e3-48a4-a9a0-dc5715e168a4-0', usage_metadata={'input_tokens': 97, 'output_tokens': 46, 'total_tokens': 143})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi I am guru and I am a data scientist\"),\n",
    "        AIMessage(content=\"Hi Guru! It's nice to meet you.  \\n\\nThat's awesome that you're a data scientist. What kind of work do you specialize in? \\n\\nI'm always interested in learning more about how people use data to solve problems and make insights.\"),\n",
    "        HumanMessage(content=\"Hey Whhat's my name what do I do?\")      \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "## This function ensures that every chat or session is different from the other\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey my name is Guru and I am data scientist\")\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Guru, it's great to meet you! \\n\\nBeing a data scientist is a fascinating field. What kind of work do you specialize in?  Do you work with a specific industry or type of data?  I'm always eager to learn more about the exciting things people are doing with data! üòä  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Guru!  üòä  \\n\\nI remember from our previous conversation. \\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 179, 'total_tokens': 200, 'completion_time': 0.038181818, 'prompt_time': 0.005295265, 'queue_time': 0.019879584999999998, 'total_time': 0.043477083}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d61f6bed-1e2a-4e39-9dda-c2f0d10f9a80-0', usage_metadata={'input_tokens': 179, 'output_tokens': 21, 'total_tokens': 200})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey What's my name?\")\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you.  \\n\\nWhat can I do for you today? üòÑ\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chage the session id\n",
    "\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey my name is john\")\n",
    "    ],\n",
    "    config=config1\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is John, remember? üòä \\n\\nI'm here to help if you need anything else!  \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What's my name\")\n",
    "    ],\n",
    "    config=config1\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a useful assistant, Answer all the question with best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Guru, it's nice to meet you!  \\n\\nI'm glad you're here. What can I do for you today?  \\n\\nAsk me anything, and I'll do my best to help. üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 29, 'total_tokens': 81, 'completion_time': 0.094545455, 'prompt_time': 0.00014056, 'queue_time': 0.020152266999999998, 'total_time': 0.094686015}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7893c99-59cd-4ea3-b060-8a1f5a053b0f-0', usage_metadata={'input_tokens': 29, 'output_tokens': 52, 'total_tokens': 81})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Guru\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Guru! It's nice to meet you. \\n\\nI'm happy to help with any questions you might have. Just ask away! üòä  \\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Guru\")],\n",
    "    config=config2\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more complexity\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a useful assistant, Answer all the question with best of your ability in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ó‡•Å‡§∞‡•Å ‡§ú‡•Ä!  \\n\\n‡§Ü‡§ú ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡§æ ‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ‡•§ üòä\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Guru\")],\"language\":\"Hindi\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this more complicated chain in messages history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ó‡•Å‡§∞‡•Å ‡§ú‡•Ä! \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ú‡•ã ‡§≠‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç‡§ó‡•á, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§∏‡§¨‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á ‡§∏‡§ï‡•Ç‡§Å‡•§ \\n\\n‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? ü§î \\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hi, I am Guru\")], \"language\":\"Hindi\"},\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§®‡•á ‡§π‡•Ä ‡§ñ‡•Å‡§¶ ‡§Ö‡§™‡§®‡•Ä ‡§™‡§π‡§ö‡§æ‡§® ‡§¨‡§§‡§æ‡§à ‡§ó‡•Å‡§∞‡•Å ‡§ú‡•Ä, ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ \"‡§ó‡•Å‡§∞‡•Å\" ‡§π‡•à‡•§  üòä \\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")], \"language\":\"Hindi\"},\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menage the conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'trim_message' = trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\rag-app\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\rag-app\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.1 MB 1.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/10.1 MB 1.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.1 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.1/10.1 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.9/10.1 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.1 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.1 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.6/10.1 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.1/10.1 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 2.9 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.12.0 huggingface-hub-0.27.0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like to play cricket', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem !', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi ! my name is lokas!\"),\n",
    "    AIMessage(content=\"Hi !\"),\n",
    "    HumanMessage(content=\"I like to play cricket\"),\n",
    "    AIMessage(content=\"Nice\"),\n",
    "    HumanMessage(content=\"Whats 2+2?\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No problem !\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a tricky one! I don't actually know what games *you* like.\\n\\nTo figure that out, tell me: \\n\\n* What kind of things do you enjoy doing? \\n* Do you prefer playing alone, with friends, or online?\\n* Are you looking for a game that's relaxing, challenging, creative, or something else?\\n\\nThe more you tell me, the better I can help you find a game you'll love! üòä  \\n\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What game I like to play?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets wraop this all in message history\n",
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config4 = {\"configurable\":{\"session_id\":\"chat5\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name.  \\n\\nIf you'd like to tell me your name, I'd be happy to use it! üòä  \\n\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")], \"language\":\"English\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please tell me the math problem you solved so I can tell you if I can help! üòä  \\n\\nI need to know the problem to confirm if you solved it correctly.  \\n\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What math's problem did I solved?\")], \"language\":\"English\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
