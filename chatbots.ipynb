{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a chatbot with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001AD1A947320>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001AD1A947E60>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Guru! It's nice to meet you.  \\n\\nThat's awesome that you're a data scientist. What kind of work do you specialize in? \\n\\nI'm always interested in learning more about how people use data to solve problems and make insights.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 19, 'total_tokens': 78, 'completion_time': 0.107272727, 'prompt_time': 8.034e-05, 'queue_time': 0.020264169999999998, 'total_time': 0.107353067}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d51881fe-de9d-4d15-9dfd-46fc5569ea21-0', usage_metadata={'input_tokens': 19, 'output_tokens': 59, 'total_tokens': 78})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi I am guru and I am a data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Guru, and you're a data scientist! 😊  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm happy to chat.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 97, 'total_tokens': 143, 'completion_time': 0.083636364, 'prompt_time': 0.0034427, 'queue_time': 0.020834526, 'total_time': 0.087079064}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c8e680da-42e3-48a4-a9a0-dc5715e168a4-0', usage_metadata={'input_tokens': 97, 'output_tokens': 46, 'total_tokens': 143})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi I am guru and I am a data scientist\"),\n",
    "        AIMessage(content=\"Hi Guru! It's nice to meet you.  \\n\\nThat's awesome that you're a data scientist. What kind of work do you specialize in? \\n\\nI'm always interested in learning more about how people use data to solve problems and make insights.\"),\n",
    "        HumanMessage(content=\"Hey Whhat's my name what do I do?\")      \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "## This function ensures that every chat or session is different from the other\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey my name is Guru and I am data scientist\")\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Guru, it's great to meet you! \\n\\nBeing a data scientist is a fascinating field. What kind of work do you specialize in?  Do you work with a specific industry or type of data?  I'm always eager to learn more about the exciting things people are doing with data! 😊  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Guru!  😊  \\n\\nI remember from our previous conversation. \\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 179, 'total_tokens': 200, 'completion_time': 0.038181818, 'prompt_time': 0.005295265, 'queue_time': 0.019879584999999998, 'total_time': 0.043477083}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d61f6bed-1e2a-4e39-9dda-c2f0d10f9a80-0', usage_metadata={'input_tokens': 179, 'output_tokens': 21, 'total_tokens': 200})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey What's my name?\")\n",
    "    ],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you.  \\n\\nWhat can I do for you today? 😄\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chage the session id\n",
    "\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hey my name is john\")\n",
    "    ],\n",
    "    config=config1\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is John, remember? 😊 \\n\\nI'm here to help if you need anything else!  \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What's my name\")\n",
    "    ],\n",
    "    config=config1\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a useful assistant, Answer all the question with best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Guru, it's nice to meet you!  \\n\\nI'm glad you're here. What can I do for you today?  \\n\\nAsk me anything, and I'll do my best to help. 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 29, 'total_tokens': 81, 'completion_time': 0.094545455, 'prompt_time': 0.00014056, 'queue_time': 0.020152266999999998, 'total_time': 0.094686015}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7893c99-59cd-4ea3-b060-8a1f5a053b0f-0', usage_metadata={'input_tokens': 29, 'output_tokens': 52, 'total_tokens': 81})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Guru\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Guru! It's nice to meet you. \\n\\nI'm happy to help with any questions you might have. Just ask away! 😊  \\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response1=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Guru\")],\n",
    "    config=config2\n",
    ")\n",
    "response1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more complexity\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a useful assistant, Answer all the question with best of your ability in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते गुरु जी!  \\n\\nआज मैं आपकी मदद करने के लिए तैयार हूँ। कोई भी प्रश्न पूछें, मैं अपना सर्वश्रेष्ठ प्रयास करूँगा। 😊\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Guru\")],\"language\":\"Hindi\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this more complicated chain in messages history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते गुरु जी! \\n\\nमुझे खुशी है कि आप मुझसे बात कर रहे हैं। आप मुझसे जो भी प्रश्न पूछेंगे, मैं अपनी पूरी कोशिश करूँगा कि आपको सबसे बेहतर उत्तर दे सकूँ। \\n\\nआप क्या जानना चाहते हैं? 🤔 \\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hi, I am Guru\")], \"language\":\"Hindi\"},\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आपने ही खुद अपनी पहचान बताई गुरु जी, आपका नाम \"गुरु\" है।  😊 \\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")], \"language\":\"Hindi\"},\n",
    "    config=config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menage the conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'trim_message' = trim_messages can be used to reduce the size of a chat history to a specified token count or specified message count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\rag-app\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\rag-app\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\rag-app\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\rag-app\\.venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.1 MB 1.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/10.1 MB 1.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.1 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.1/10.1 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.9/10.1 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.1 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.1 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.6/10.1 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.1/10.1 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 2.9 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.12.0 huggingface-hub-0.27.0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like to play cricket', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Whats 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem !', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi ! my name is lokas!\"),\n",
    "    AIMessage(content=\"Hi !\"),\n",
    "    HumanMessage(content=\"I like to play cricket\"),\n",
    "    AIMessage(content=\"Nice\"),\n",
    "    HumanMessage(content=\"Whats 2+2?\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No problem !\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a tricky one! I don't actually know what games *you* like.\\n\\nTo figure that out, tell me: \\n\\n* What kind of things do you enjoy doing? \\n* Do you prefer playing alone, with friends, or online?\\n* Are you looking for a game that's relaxing, challenging, creative, or something else?\\n\\nThe more you tell me, the better I can help you find a game you'll love! 😊  \\n\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What game I like to play?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets wraop this all in message history\n",
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "config4 = {\"configurable\":{\"session_id\":\"chat5\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name.  \\n\\nIf you'd like to tell me your name, I'd be happy to use it! 😊  \\n\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What's my name?\")], \"language\":\"English\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please tell me the math problem you solved so I can tell you if I can help! 😊  \\n\\nI need to know the problem to confirm if you solved it correctly.  \\n\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What math's problem did I solved?\")], \"language\":\"English\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
